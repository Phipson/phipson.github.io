/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
///// 3D INTERFACES
/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
CAVECAD:
Subtitle: A collaborative CAD Tool for designing flat-pack furniture

Overview (Design Problem):
Commercial CAD Software offer limited opportunities for end users to design products collaboratively. To design 3D objects using a 2D user interface, users typically have to surmount a steep learning curve, comprised of learning to navigate and employ software-specific tools using a keyboard and mouse, which are nonintuitive modes of input dissimilar to conventional design interactions involving pencil and paper. Additionally, commercial CAD Software offer little physical reference points: to prototype and evaluate the functionality of their models, users would have to manufacture their designs, perform tests, and modify their designs where necessary. This recurring cycle of testing and designing imposes a bottleneck in typical CADCAM workflow, and is particularly pervasive with furniture design, where designers have to work with clients to create products that are aesthetically pleasing, functional, and ergonomically suitable for the user.

Advancements in VR and mobile AR addresses some of these common challenges, by offering access to expressive modes of input such as 3D hand gestures and voice, as well as access to a 3D virtual environment scaled to real world dimensions. To overcome the limitations of collaborative CAD, we introduce a Collaborative Augmented and Virtual Environment for Computer-Aided Design (CAVECAD), which is a CAD tool that offers a 3D collaborative design environment in VR and mobile AR, for end users to design fabricable flat-pack furniture collaboratively or independently, in the same or remote environments, simultaneously or at different points in time.

User Profiles and Personas:
To motivate the need for 3D collaborative CAD tools in furniture design, I brainstormed and introduced several user profiles who would use or benefit from collaborative CAD tools in AR and VR.
1. Edith: A professional designer new to CAD but with prior experience designing with pencil and paper. As part of team project, she wants to send designs to 
2. Carl: Client who is working remotely with expert designer to create a customized table to fit corner of new house. Wants to offer feedback and suggestions to current design but difficult to modify or describe via call
3. Kathy: Designer who is working as part of a team and working with manufacturers. 
4. Jason: Designing new office but would like external feedback and support. However, due to time zone differences, it is impossible to meet and discuss project at the same time, so needs mechanism of showcasing content and commenting on projects at different points in time.

Design Process:
The research project can be divided into several stages:
1. Designing an API
2. Design a unified Gesture Based Interface for AR/VR to build furniture
3. Prototyping an interface in mobile AR and VR
4. Engineering a networking interface to communicate between mobile AR and VR environments
5. Conducting user studies to evaluate the collaborative design workflow in AR and VR

Prototypes and Videos:
*TODO: Update Video*

Other Work Links:
1. Overleaf
2. Project Proposal

Evaluation:

Final Thoughts:

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
(For Apple-related work, can say "As my work is under Apple's NDA I am not allowed to describe project in detail. However, if you would like more information, you can contact me via email, and I can connect you to my manager who oversaw my work")
LegoLogic
Subtitle: 

Foreword: 
As my work is under Apple's NDA, I am not allowed to describe the project in detail. However, my work was demoed live at internal Open House events, and videos of my work have been showcased across different teams at Apple.

Overview (Design Problem):
As part of my summer internship at Apple's Applied Research Team, I initiated a logic programming research project, investigating ways users could program logic for virtual content using ARKit- Apple's augmented reality library. The motivation for this is to introduce a programming environment directly in AR, so that users can avoid the tedium of jumping back and forth between their 2D programming interface and 3D AR environment, thus making logic programming in 3D space more efficient and streamlined.

As part of this project, I used the Apple iPad as a primary instrument for interacting in ARKit. I also experimented with 3D Hand Gestures using the LeapMotion Camera, which was attached to the iPad and integrated into the ARKit environment. Additionally, after several design iterations, I later integrated a voice library with the existing logic interface so users could program purely by speaking.

User Profiles:
To motivate the development of logic-based programming in AR, I brainstormed and examined logic commands users may want to design and use for AR Content using Reality Composer. Among the different use cases, I observed that most logic commands and statements can easily be represented as English sentences, shown in the following two examples.

1. Dog jumps every 10 seconds.
2. Robot grabs big, juicy apples.

Related Work and Inspiration:
MIT Scratch
Literature Review

Design Process:
1. Designing Grammar and Logic Programming Language
    - Observing the typical goals and commands users would use when implementing logic in AR, I approached the project by mapping logical components to English grammar and parts of speech
    - The reason for doing so would be to abstract the underlying complexity of the programming language, and instead, focus on the 
2. Translating Design into AR Environment
3. Adding voice networking capabilities with program

User Testing and Modifications:
- Performed informal user studies over the course of the project, asking users to piece together certain commands and logic to objects in a prefabricated AR scene
- Qualitative observations were made: asking questions, asking users to narrate their thought process (akin to a Think Aloud Protocol)
- Participants were all Apple employees or interns with variable programming background and knowledge

Evaluation:
- With this project, I tackled logic programming from a top down approach: I first examined common commands and intents users would have, and found ways of directly representing those said intents into a logic interface
- Although this approach made the interface more accessible, intuitive, and convenient for casual end users, as it uses a skeuomorphic design that draws analogies from physical interactions with lego blocks, there were implementation and scalability challenges.
- My greatest takeaway from this experience was to clearly define the scope of the project before designing the interface: one of the challenges to this existing model was that, as I did not clearly define the scope of this project, the logic interface would not work correctly for extended, complex logic and actions. This was partially also due in part to the complexity of the English language, where words can be used in different contexts, which would thereby break the rules of the existing logic.
- Although I was able to address some of these issues by adding case-specific scripts, as I did not clearly define the components of the system, it thereby became a challenge to later integrate the visual representation of the logic with the voice library

Final Thoughts and Future Work:
- Rewarding learning experience that taught me the process of transforming design ideas into prototypes and products that align with the company's goals. Taking this learning experience with me, I continue to expand upon my logic programming interface by co-leading a research project investigating programming using voice in AR. 

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
VARLog:
Subtitle:

Overview:
Apple's newly released Reality Composer offers new features for end-users to animate and interact with virtual content using ARKit. However, how would Reality Composer look like in an AR environment? Towards the end of my summer internship at Apple, I worked with a group of 3D software engineers at Apple's Applied Research Team to explore and investigate ways we could use voice to interact with Reality Composer. In contrast to my original logic programming interface, which used 3D hand gestures as a primary mode of input, our goal was to identify ways to program strictly using voice, and design a scalable representation of behaviors and actions in Reality Composer.

Currently, Reality Composer does not offer ample utilities to efficiently prototype and debug projects in an AR/VR environment. As a result, users have to navigate the logic of their program using typical 2D gestures or keyboard and mouse interface. On the other hand, voice offers users more expressive capabilities that are familiar and intuitive for casual end users with little programming experience. Rather than representing an action or behavior in terms of a programming language, we propose using voice input to generate code by saying the intended action to be implemented

Profiles and Use Cases:
To define the scope of our project, we first brainstormed different possible use cases for Reality Composer. We then categorized different these scenarios based on target audience and the intended product. For preliminary research purposes, we then selected a small portion of the categories most representative of our target audience as the basis of our research. The following user profiles resulted from the brainstorming session:

Bowling Ball Game
Virtual Apple Store
AR Fast Food Menu

To represent the logic for each of the scenarios, we went with a bottom-up approach: we distilled the common components of each logical command in Reality Composer, and from there, identified the types of possible voice commands that users could utter to express these components. After our brainstorming sessions, we derived the following 3 classes of data we could use to represent behaviors in Reality Composer.

Actions: The observed behavior in the scene.

Trigger: An event that precedes the action.

Condition: An internal state or property of an object or scene that must be held for an action to occur.

In tandem with our brainstorming sessions, we also designed and piloted user studies investigating voice commands users would use to represent an intended behavior.

Related Work and Inspiration:
To give users visual feedback of their voice input, we proceeded by investigating different representations of the logic. One constraint we have is that the visual representation of the logic must convey the underlying modes of interaction: as our research is focused on voice input, we must avoid visuals that suggest hand gestures are the primary mode of input. As a result, my goal was to eschew block or graph-based representations for our interface. 

Google Game Builder
Scratch

Design Process:
- Represented voice commands as play cards, containing a visual graphic, a text representation of the voice input
- As the card is entirely voice generated, there is no block/node graph representation that imply logic can be generated using hand gestures
- To maximize the capabilities of 3D space, users could collapse details of the card by tapping on the card
    - Hand gestures now act as a secondary mode of input to organize the virtual programming workspace, where users can hide and expand the underlying complexities of the logic

Due to time constraints, I left Apple during the middle of the research. However, I was able to showcase code and videos of my ongoing interface and sketches illustrating concepts and workflows with the intended interface.

Ongoing Work and Evaluation:
- If I had more time, I would like to design methods for debugging and offering visual feedback for users when they are selecting and modifying specific parts of their project
    - Designing methods of isolating parts of the workspace and organizing the workspace to show and hide specific parts of the logic
    - For larger projects, it would also be useful to identify visual feedback for identifying selected objects and how logic is attached or connected to objects in the scene
- Continue running user studies
1. Integrating and modifying Voice Input Interface
2. Examining and evaluating user study data

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Voxel Modeling Tool:
Subtitle:

Overview:
As part of a research activity at Apple's Applied Research Team, I worked with different engineers to investigate methods users could create personalized assets in AR. Currently, one of the challenges when using Reality Composer is that casual designers and end users do not get the opportunity to directly build and create their own 3D models to interact with. Instead, they must use pre-existing models for their composition, or design assets using external software. To improve the workflow and creative capabilities of Reality Composer, I investigated ways users could maximize the capabilities of voxel modeling to create models using free-form, expressive hand gestures.

In the process of designing this interface, I used the AR capabilities on the latest iPad. I also captured 3D Hand Gestures by combining the ARKit environment with LeapMotion sensors. 

Profiles and Use Cases:
To explore different 3D modeling, the group came together for multiple brainstorming sessions examining different possible use cases and design goals. After narrowing down the scope of our project, I designed the following user profiles repesenting the different scenarios users would want to design assets for Reality Compoosers, these user profiles are both from personal experiences using 3D modeling CAD software, existing literature surrounding CAD software, as well as other group members' experiences with 3D modeling.


From these user profiles I identified the following design challenges most common with existing 3D designers:
1. Accuracy and precision with free form gestures
2. Dynamic workflow for generating large volumes and modifying details 

Here, we are assuming that end users have some design experience and thus are capable of designing their own virtual content in 3D space.

Related Work and Inspiration:
ZBrush
TiltBrush
Dreams
Minecraft
Goxel
MagicaVoxel

Design Process:
1. Designing a Workflow and identifying key functions that I would like to incorporate into my prototype:
    - Creating voxels
    - Creating primitive shapes: cubes, spheres, lines
    - Erasing voxels
    - Changing color
    - Navigating scene
    - Extruding face
2. Designing Gesture Interface to map to functions/tools
    - Pinch
    - Tap iPad screen to select mode
3. Introducing asynchronous mesh voxelization
4. User studies and feedback

Evaluation:
- Demo project, no opportunity for user studies or evaluation
- Scalability and performance is issue as frame rate drops significantly after creating certain number of voxels
- Also issue with asynchronous voxelization of meshes and extrusion
- Scalability of menu (how to organize tools in a meaningful way in 3D space)

Final Thoughts and Future Work:

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
///// WEB MOBILE DEV
/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
3D4E (3D Printing for Everyone)
UCLA's 3D Printing Society

Overview:
Originally a general member of UCLA's 3D printing society, I joined the executive board of 3D4E as the webmaster to enhance the club's branding and online presence. Specifically, previous iterations of the club website had several loopholes:
1. navigation challenges
2. lack of professional identity or branding

As a group effort to improve club outreach and organize and intercollegiate 3D printing showcase with other 3D printing clubsin California, I spearheaded a new initiative to improve the club website UI/UX.

Profile and Use Cases:

Related Work and Inspiration:

Design Process:
1. Sketches and Prototypes
2. Preliminary videoed concepts and peer evaluation
3. Implementation
4. Adding an eShop (ongoing)

Evaluation and Final Thoughts:

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
CS 188 - Scalable Internet Services
Subtitle:

Overview:

Profile and Use Cases:

Related Work and Inspiration:

Design Process:

Evaluation and Final Thoughts:

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
SUS
Subtitle:

Overview:

Profile and Use Cases:

Related Work and Implementation:

Design Process:

Evaluation and Final Thoughts:

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
///// RESEARCH
/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
UCLA HCI Lab - Genos
Subtitle:

Overview: 

SubProjects:
1. CAVECAD
2. Geno (Motivation, Literature Studies, Prototyping, Formative User Studies)

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
UCLA VCLA - VRGym
Subtitle:

Overview:

SubProjects:
1. Modifying Meshes
2. Integrating and Creating WebGL Implementation
3. IQA Dataset
4. User Study
5. Intuitive Physics Simulation Environment

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
CUHK Self Regulation Lab
Subtitle:

Overview:

SubProjects:
1. Regulation App
2. Conducting and Designing Behavioral Studies

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
HKBN
Subtitle:

Overview:

SubProjects:
1. Executing User Study
2. Designing Internal Web Application for Data Visualization and Management